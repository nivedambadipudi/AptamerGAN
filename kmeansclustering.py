# -*- coding: utf-8 -*-
"""Kmeansclustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uV1VsoM9K9mMBoPlvsRq79EODibkTFHE
"""

import torch
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

dna_alphabet = ['A', 'T', 'C', 'G']

sequence_length = 30
input_dim = len(dna_alphabet)
output_dim = len(dna_alphabet)

# Define the Generator class
class Generator(torch.nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.fc = torch.nn.Linear(input_dim, 128 * sequence_length)
        self.res_blocks = torch.nn.Sequential(
            ResBlock(128, 256),
            ResBlock(256, 256),
            ResBlock(256, 512),
            ResBlock(512, 512)
        )
        self.output_layer = torch.nn.Linear(512 * sequence_length, sequence_length * output_dim)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.fc(x)
        x = x.view(x.size(0), 128, sequence_length)
        x = self.res_blocks(x)
        x = x.view(x.size(0), -1)
        x = self.output_layer(x)
        x = self.sigmoid(x)
        x = x.view(x.size(0), sequence_length, output_dim)
        return x

class ResBlock(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ResBlock, self).__init__()
        self.conv1 = torch.nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.bn1 = torch.nn.BatchNorm1d(out_channels)
        self.relu = torch.nn.ReLU(inplace=True)
        self.conv2 = torch.nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.bn2 = torch.nn.BatchNorm1d(out_channels)
        self.shortcut = torch.nn.Sequential()
        if in_channels != out_channels:
            self.shortcut = torch.nn.Sequential(
                torch.nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1),
                torch.nn.BatchNorm1d(out_channels)
            )

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        residual = self.shortcut(residual)
        out += residual
        out = self.relu(out)
        return out

generator_paths = ["Generator_theo06.txt.pt", "Generator_theo10.txt.pt", "Generator_theo12.txt.pt",
                   "Generator_theo15.txt.pt", "Generator_theo16.txt.pt", "Generator_theo18.txt.pt",
                   "Generator_theo20.txt.pt", "Generator_theo22.txt.pt"]

for generator_path in generator_paths:
    # Load the model
    generator = torch.load(generator_path, map_location=torch.device('cpu'))

    # If the loaded object is not a PyTorch model but a state_dict
    if isinstance(generator, dict):
        # Create the Generator model instance
        generator = Generator()
        # Load the state_dict
        generator.load_state_dict(torch.load(generator_path, map_location=torch.device('cpu')))

    # Set to evaluation mode
    generator.eval()

    def generate_random_input(batch_size, input_dim):
        return torch.FloatTensor(np.random.uniform(0, 1, (batch_size, input_dim)))

    def generate_sequences(generator, num_samples):
        with torch.no_grad():
            z = generate_random_input(num_samples, input_dim)
            fake_seqs = generator(z)
        return fake_seqs

    num_generated = 100
    generated_seqs = generate_sequences(generator, num_generated)

    decoded_seqs = []
    for seq in generated_seqs:
        decoded_seq = ''.join([dna_alphabet[i] for i in torch.argmax(seq, dim=1)])
        decoded_seqs.append(decoded_seq)

    seq_strings = [''.join([dna_alphabet[i] for i in torch.argmax(seq, dim=1)]) for seq in generated_seqs]

    vectorizer = CountVectorizer(analyzer='char')
    X = vectorizer.fit_transform(seq_strings)

    best_score = -1
    best_k = 2
    for k in range(2, 11):
        kmeans = KMeans(n_clusters=k, random_state=0)
        cluster_labels = kmeans.fit_predict(X)
        silhouette_avg = silhouette_score(X, cluster_labels)
        print(f"Number of clusters: {k}, Silhouette Score: {silhouette_avg}")
        if silhouette_avg > best_score:
            best_score = silhouette_avg
            best_k = k

    print(f"Best number of clusters: {best_k}")

    kmeans = KMeans(n_clusters=best_k, random_state=0)
    clusters = kmeans.fit_predict(X.toarray())

    data = {'Sequence': decoded_seqs, 'Cluster': clusters}
    df = pd.DataFrame(data)

    csv_file = 'KmeansGan{}.csv'.format(generator_path)
    df.to_csv(csv_file, index=False)

    print(f"Clusters saved to {csv_file}")